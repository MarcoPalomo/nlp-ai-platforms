#!/bin/bash

# Script de conversion et packaging du modÃ¨le Mistral pour TorchServe
# Usage: ./convert_model.sh [model_path] [output_dir]

set -e

# Configuration par dÃ©faut
MODEL_NAME="mistral-7b-instruct"
MODEL_PATH=${1:-"mistralai/Mistral-7B-Instruct-v0.1"}
OUTPUT_DIR=${2:-"../torchserve/model-store"}
HANDLER_PATH="../torchserve/custom_handler.py"
CONFIG_PATH="config/config.json"

echo "ðŸš€ Conversion du modÃ¨le Mistral pour TorchServe"
echo "Model: $MODEL_PATH"
echo "Output: $OUTPUT_DIR"

# VÃ©rification des dÃ©pendances
check_dependencies() {
    echo "ðŸ“¦ VÃ©rification des dÃ©pendances..."
    
    if ! command -v torch-model-archiver &> /dev/null; then
        echo "âŒ torch-model-archiver non trouvÃ©. Installation..."
        pip install torchserve torch-model-archiver torch-workflow-archiver
    fi
    
    python -c "import transformers" 2>/dev/null || {
        echo "âŒ transformers non trouvÃ©. Installation..."
        pip install transformers accelerate bitsandbytes
    }
    
    echo "âœ… DÃ©pendances vÃ©rifiÃ©es"
}

# TÃ©lÃ©chargement et prÃ©paration du modÃ¨le
prepare_model() {
    echo "ðŸ“¥ PrÃ©paration du modÃ¨le..."
    
    # CrÃ©ation du rÃ©pertoire temporaire
    TEMP_DIR="/tmp/mistral_conversion"
    rm -rf $TEMP_DIR
    mkdir -p $TEMP_DIR
    
    # Script Python pour tÃ©lÃ©charger et sauvegarder le modÃ¨le
    cat > $TEMP_DIR/download_model.py << 'EOF'
import os
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json

model_path = sys.argv[1]
output_path = sys.argv[2]

print(f"TÃ©lÃ©chargement du modÃ¨le: {model_path}")

try:
    # Chargement du tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Chargement du modÃ¨le
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="cpu",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # Sauvegarde
    os.makedirs(output_path, exist_ok=True)
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    # CrÃ©ation du fichier de mÃ©tadonnÃ©es
    metadata = {
        "model_name": "mistral-7b-instruct",
        "model_path": model_path,
        "tokenizer_class": tokenizer.__class__.__name__,
        "model_class": model.__class__.__name__,
        "torch_dtype": "float16",
        "vocab_size": tokenizer.vocab_size,
        "max_position_embeddings": getattr(model.config, 'max_position_embeddings', 4096)
    }
    
    with open(os.path.join(output_path, "model_metadata.json"), "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"âœ… ModÃ¨le sauvegardÃ© dans: {output_path}")
    
except Exception as e:
    print(f"âŒ Erreur lors du tÃ©lÃ©chargement: {e}")
    sys.exit(1)
EOF

    python $TEMP_DIR/download_model.py "$MODEL_PATH" "$TEMP_DIR/model"
    echo "âœ… ModÃ¨le prÃ©parÃ©"
}

# CrÃ©ation de l'archive MAR
create_mar_archive() {
    echo "ðŸ“¦ CrÃ©ation de l'archive MAR..."
    
    # CrÃ©ation du rÃ©pertoire de sortie
    mkdir -p $OUTPUT_DIR
    
    # Suppression de l'ancienne archive si elle existe
    rm -f $OUTPUT_DIR/${MODEL_NAME}.mar
    
    # CrÃ©ation de l'archive avec torch-model-archiver
    torch-model-archiver \
        --model-name $MODEL_NAME \
        --version 1.0 \
        --serialized-file $TEMP_DIR/model/pytorch_model.bin \
        --handler $HANDLER_PATH \
        --extra-files "$TEMP_DIR/model/config.json,$TEMP_DIR/model/tokenizer.json,$TEMP_DIR/model/tokenizer_config.json,$TEMP_DIR/model/special_tokens_map.json,$CONFIG_PATH,$TEMP_DIR/model/model_metadata.json" \
        --export-path $OUTPUT_DIR \
        --force
    
    echo "âœ… Archive MAR crÃ©Ã©e: $OUTPUT_DIR/${MODEL_NAME}.mar"
}

# Validation de l'archive
validate_archive() {
    echo "ðŸ” Validation de l'archive..."
    
    if [ -f "$OUTPUT_DIR/${MODEL_NAME}.mar" ]; then
        file_size=$(du -h "$OUTPUT_DIR/${MODEL_NAME}.mar" | cut -f1)
        echo "âœ… Archive crÃ©Ã©e avec succÃ¨s ($file_size)"
        
        # Test de l'archive avec torch-model-archiver
        torch-model-archiver --export-path /tmp/test_extract --extract $OUTPUT_DIR/${MODEL_NAME}.mar
        echo "âœ… Archive validÃ©e"
        rm -rf /tmp/test_extract
    else
        echo "âŒ Erreur: Archive non crÃ©Ã©e"
        exit 1
    fi
}

# Nettoyage
cleanup() {
    echo "ðŸ§¹ Nettoyage..."
    rm -rf $TEMP_DIR
    echo "âœ… Nettoyage terminÃ©"
}

# Fonction principale
main() {
    echo "========================================="
    echo "  Conversion Mistral pour TorchServe"
    echo "========================================="
    
    check_dependencies
    prepare_model
    create_mar_archive
    validate_archive
    cleanup
    
    echo ""
    echo "ðŸŽ‰ Conversion terminÃ©e avec succÃ¨s!"
    echo "Archive disponible: $OUTPUT_DIR/${MODEL_NAME}.mar"
    echo ""
    echo "Prochaines Ã©tapes:"
    echo "1. DÃ©marrer TorchServe: torchserve --start --model-store $OUTPUT_DIR"
    echo "2. Enregistrer le modÃ¨le: curl -X POST \"localhost:8081/models?url=${MODEL_NAME}.mar\""
    echo "3. Tester: curl -X POST \"localhost:8080/predictions/${MODEL_NAME}\" -H \"Content-Type: application/json\" -d '{\"text\": \"Hello, how are you?\"}'"
}

# Gestion des erreurs
trap cleanup ERR

# ExÃ©cution
main "$@"