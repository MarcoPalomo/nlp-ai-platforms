# Configuration TorchServe pour Mistral NLP Platform
# Date: $(date +%Y-%m-%d)

# Adresses de service
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Configuration gRPC
grpc_inference_port=7070
grpc_management_port=7071

# Répertoire des modèles
model_store=./model-store

# Configuration des workers
default_workers_per_model=2
max_workers=8
min_workers=1

# Configuration de la mémoire et performance
initial_worker_port=9000
max_request_size=67108864
max_response_size=67108864
default_response_timeout=300
unregister_model_timeout=120
decode_input_request=true

# Configuration GPU/CPU
number_of_gpu=1
number_of_netty_threads=32
netty_client_threads=0
default_service_handler=service:handle

# Configuration des logs
enable_envvars_config=true
install_py_dep_per_model=false
enable_metrics_api=true
metrics_format=prometheus

# Configuration du modèle par défaut
models={\
  "mistral-7b-instruct": {\
    "1.0": {\
      "defaultVersion": true,\
      "marName": "mistral-7b-instruct.mar",\
      "minWorkers": 1,\
      "maxWorkers": 4,\
      "batchSize": 1,\
      "maxBatchDelay": 100,\
      "responseTimeout": 300\
    }\
  }\
}

# Configuration avancée
model_config_file=models.yaml
load_models=all
job_queue_size=1000

# Configuration CORS pour les appels cross-origin
cors_allowed_origin=*
cors_allowed_methods=GET,POST,PUT,DELETE,OPTIONS
cors_allowed_headers=*

# Configuration SSL (optionnel)
# keystore=keystore.p12
# keystore_pass=changeit
# keystore_type=PKCS12

# Configuration de sécurité
allowed_urls=file://,http://,https://
blacklist_env_vars=

# Configuration du snapshot
snapshot_disabled=false
model_snapshot={\"disabled\":false}

# Optimisations performance
async_logging=true
prefer_direct_buffer=true
restricted_memory=false

# Configuration workflow (optionnel)
workflow_store=./workflow-store