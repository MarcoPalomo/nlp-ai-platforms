# ğŸ¤– Plateforme NLP avec Mistral et TorchServe

Plateforme complÃ¨te d'orchestration NLP utilisant Mistral-7B-Instruct via TorchServe avec une API REST FastAPI.

## ğŸ“‹ Table des matiÃ¨res

- [ğŸš€ DÃ©marrage rapide](#-dÃ©marrage-rapide)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ“¦ Installation](#-installation)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸš€ Utilisation](#-utilisation)
- [ğŸ“¡ API Endpoints](#-api-endpoints)
- [ğŸ“Š Monitoring](#-monitoring)
- [ğŸ³ Docker](#-docker)
- [ğŸ› ï¸ DÃ©veloppement](#ï¸-dÃ©veloppement)
- [ğŸ“š Documentation](#-documentation)

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

- Docker & Docker Compose
- Python 3.10+
- CUDA-compatible GPU (optionnel mais recommandÃ©)
- 16GB+ RAM
- 50GB+ espace disque

### Installation en 1 commande

```bash
# Clone et dÃ©ployement complet
git clone <your-repo>
cd nlp-platform-mistral
chmod +x deploy.sh
./deploy.sh dev setup
```

### Test rapide

```bash
# Test de gÃ©nÃ©ration de texte
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{"text": "Explique-moi l'\''IA en 3 phrases", "max_tokens": 200}'
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚   Client Web    â”‚â—„â”€â”€â–ºâ”‚   FastAPI        â”‚â—„â”€â”€â–ºâ”‚   TorchServe    â”‚
â”‚   /Mobile/API   â”‚    â”‚   Orchestrator   â”‚    â”‚   + Mistral     â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚                  â”‚
                       â”‚   Redis Cache    â”‚
                       â”‚   + Monitoring   â”‚
                       â”‚                  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants principaux

1. **TorchServe** : Serveur de modÃ¨les ML avec Mistral-7B-Instruct
2. **FastAPI Orchestrator** : API REST avec gestion des requÃªtes, cache, mÃ©triques
3. **Redis** : Cache distribuÃ© pour les rÃ©ponses
4. **Prometheus + Grafana** : Monitoring et mÃ©triques
5. **Custom Handler** : Gestionnaire optimisÃ© pour Mistral

## ğŸ“¦ Installation

### 1. Installation manuelle

```bash
# Clone du repository
git clone <your-repo>
cd nlp-platform-mistral

# Installation des dÃ©pendances
pip install -r requirements.txt

# Configuration des modÃ¨les
cd mistral
chmod +x convert_model.sh
./convert_model.sh

# DÃ©marrage de TorchServe
cd ../torchserve
torchserve --start --model-store model-store --ts-config config.properties

# DÃ©marrage de l'orchestrateur
python api_server.py
```

### 2. Installation avec Docker

```bash
# DÃ©marrage avec Docker Compose
docker-compose up -d

# VÃ©rification
docker-compose ps
curl http://localhost:8000/health
```

## ğŸ”§ Configuration

### Fichiers de configuration principaux

#### `mistral/config/config.json`
Configuration du modÃ¨le Mistral avec paramÃ¨tres de gÃ©nÃ©ration.

#### `torchserve/config.properties`
Configuration TorchServe (workers, mÃ©moire, GPU).

#### `torchserve/custom_handler.py`
Handler personnalisÃ© avec optimisations pour Mistral.

#### `docker-compose.yml`
Orchestration des services Docker.

### Variables d'environnement

```bash
# TorchServe
TORCHSERVE_URL=http://localhost:8080

# API
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
MAX_WORKERS=10

# GPU
CUDA_VISIBLE_DEVICES=0
```

## ğŸš€ Utilisation

### DÃ©marrage des services

```bash
# Environnement de dÃ©veloppement
./deploy.sh dev start

# Environnement de production
./deploy.sh prod start

# VÃ©rification du status
./deploy.sh dev status
```

### Commandes utiles

```bash
# Logs en temps rÃ©el
./deploy.sh dev logs

# Test de fonctionnalitÃ©
./deploy.sh dev test

# Restart des services
./deploy.sh dev restart

# Sauvegarde
./deploy.sh dev backup

# Nettoyage
./deploy.sh dev cleanup
```

## ğŸ“¡ API Endpoints

### Base URL: `http://localhost:8000`

### SantÃ© et status

```http
GET /health          # VÃ©rification de santÃ©
GET /status          # Status complet du systÃ¨me
GET /metrics         # MÃ©triques de performance
GET /models          # ModÃ¨les disponibles
```

### GÃ©nÃ©ration de texte

```http
POST /generate
Content-Type: application/json

{
  "text": "Votre prompt ici",
  "max_tokens": 512,
  "temperature": 0.7,
  "top_p": 0.9,
  "top_k": 50,
  "repetition_penalty": 1.1
}
```

### Question-RÃ©ponse

```http
POST /question-answering
Content-Type: application/json

{
  "question": "Qu'est-ce que l'intelligence artificielle?",
  "context": "L'IA est une technologie...",
  "max_tokens": 300
}
```

### RÃ©sumÃ© de texte

```http
POST /summarize
Content-Type: application/json

{
  "text": "Texte long Ã  rÃ©sumer...",
  "max_length": 150,
  "temperature": 0.3
}
```

### Traduction

```http
POST /translate
Content-Type: application/json

{
  "text": "Hello, how are you?",
  "target_language": "franÃ§ais",
  "max_tokens": 100
}
```

### Classification

```http
POST /classify
Content-Type: application/json

{
  "text": "Ce produit est fantastique!",
  "categories": ["positif", "nÃ©gatif", "neutre"]
}
```

### Traitement par batch

```http
POST /batch
Content-Type: application/json

{
  "requests": [
    {
      "text": "PremiÃ¨re requÃªte",
      "task_type": "text_generation",
      "parameters": {"max_tokens": 100}
    },
    {
      "text": "DeuxiÃ¨me requÃªte", 
      "task_type": "question_answering",
      "parameters": {"max_tokens": 150}
    }
  ],
  "priority": 2
}
```

## ğŸ“Š Monitoring

### MÃ©triques disponibles

- **Performance** : Temps de rÃ©ponse, throughput, latence
- **Utilisation** : CPU, GPU, mÃ©moire
- **Cache** : Taux de hit, taille du cache
- **Erreurs** : Taux d'erreur, types d'erreurs
- **ModÃ¨les** : Utilisation des modÃ¨les, versions

### Dashboards

- **Grafana** : http://localhost:3000 (admin/admin123)
- **Prometheus** : http://localhost:9090
- **TorchServe Metrics** : http://localhost:8082

### Logs

```bash
# Logs de l'API
docker-compose logs nlp-orchestrator

# Logs de TorchServe
docker-compose logs torchserve

# Logs temps rÃ©el
docker-compose logs -f --tail=100
```

## ğŸ³ Docker

### Images utilisÃ©es

- `pytorch/torchserve:latest-gpu` : TorchServe avec support GPU
- `python:3.10-slim` : Base pour l'orchestrateur
- `redis:7-alpine` : Cache Redis
- `prom/prometheus:latest` : MÃ©triques
- `grafana/grafana:latest` : Dashboards

### Volumes

```yaml
volumes:
  - ./torchserve/model-store:/home/model-server/model-store
  - ./logs:/app/logs
  - prometheus_data:/prometheus
  - grafana_data:/var/lib/grafana
  - redis_data:/data
```

### Network

RÃ©seau Docker `172.20.0.0/16` pour communication inter-services.

## ğŸ› ï¸ DÃ©veloppement

### Structure du projet

```
nlp-platform-mistral/
â”œâ”€â”€ mistral/
â”‚   â”œâ”€â”€ config/config.json
â”‚   â””â”€â”€ convert_model.sh
â”œâ”€â”€ torchserve/
â”‚   â”œâ”€â”€ config.properties
â”‚   â”œâ”€â”€ custom_handler.py
â”‚   â””â”€â”€ model-store/
â”œâ”€â”€ orchestration_client.py
â”œâ”€â”€ api_server.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.orchestrator
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ deploy.sh
â””â”€â”€ README.md
```

### DÃ©veloppement local

```bash
# Installation en mode dÃ©veloppement
pip install -r requirements.txt
pip install -e .

# Variables d'environnement
export TORCHSERVE_URL=http://localhost:8080
export LOG_LEVEL=DEBUG

# DÃ©marrage en mode debug
python api_server.py --reload
```

### Tests

```bash
# Tests unitaires
pytest tests/

# Tests d'intÃ©gration
pytest tests/integration/

# Tests de charge
pytest tests/load/

# Coverage
pytest --cov=src tests/
```

### Contribution

1. Fork du repository
2. CrÃ©ation d'une branche feature
3. Tests et documentation
4. Pull request avec description dÃ©taillÃ©e

## ğŸ“š Documentation

### API Documentation

- **OpenAPI/Swagger** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc

### Exemples de code

#### Python

```python
import asyncio
from orchestration_client import NLPPlatformAPI, NLPRequest, TaskType

async def main():
    api = NLPPlatformAPI()
    
    # GÃ©nÃ©ration simple
    response = await api.generate_text(
        "Explique-moi l'IA",
        max_tokens=200
    )
    print(response.generated_text)

asyncio.run(main())
```

#### cURL

```bash
# GÃ©nÃ©ration de texte
curl -X POST "http://localhost:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{"text": "Bonjour", "max_tokens": 100}'

# Avec authentification (si activÃ©e)
curl -X POST "http://localhost:8000/generate" \
     -H "Authorization: Bearer your-token" \
     -H "Content-Type: application/json" \
     -d '{"text": "Bonjour"}'
```

#### JavaScript/Node.js

```javascript
const axios = require('axios');

async function generateText(prompt) {
  try {
    const response = await axios.post('http://localhost:8000/generate', {
      text: prompt,
      max_tokens: 200,
      temperature: 0.7
    });
    
    return response.data.data.generated_text;
  } catch (error) {
    console.error('Erreur:', error.response.data);
  }
}
```

## ğŸ”§ Troubleshooting

### ProblÃ¨mes courants

#### TorchServe ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker-compose logs torchserve

# VÃ©rifier l'espace disque
df -h

# VÃ©rifier la mÃ©moire
free -h

# Restart du service
docker-compose restart torchserve
```

#### Erreur GPU

```bash
# VÃ©rifier NVIDIA Docker
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# VÃ©rifier les drivers
nvidia-smi

# Mode CPU forcÃ©
export CUDA_VISIBLE_DEVICES=""
```

#### ModÃ¨le non trouvÃ©

```bash
# Reconvertir le modÃ¨le
cd mistral
./convert_model.sh

# VÃ©rifier le model store
ls -la torchserve/model-store/

# RÃ©enregistrer le modÃ¨le
curl -X POST "http://localhost:8081/models?url=mistral-7b-instruct.mar"
```

#### ProblÃ¨mes de performance

```bash
# MÃ©triques systÃ¨me
docker stats

# MÃ©triques applicatives
curl http://localhost:8000/metrics

# Optimisation GPU
export OMP_NUM_THREADS=8
export CUDA_LAUNCH_BLOCKING=1
```

### Support

- **Issues GitHub** : Rapporter les bugs et demandes de fonctionnalitÃ©s
- **Discussions** : Questions gÃ©nÃ©rales et aide
- **Wiki** : Documentation technique approfondie

## ğŸ“„ Licence

MIT License - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸš€ Roadmap

- [ ] Support multi-modÃ¨les (Llama2, Falcon, etc.)
- [ ] Interface web React
- [ ] API Gateway avec rate limiting
- [ ] DÃ©ploiement Kubernetes
- [ ] Support de fine-tuning
- [ ] IntÃ©gration avec vector databases
- [ ] Plugin systÃ¨me pour extensibilitÃ©

---

**Version** : 1.0.0  
**DerniÃ¨re mise Ã  jour** : Septembre 2025